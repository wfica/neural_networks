{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch introduction\n",
    "\n",
    "(Py)Torch is a neural network package developed by Facebook, that is extremely simple. We'll use it to implement neural networks\n",
    "\n",
    "# Torch tensor\n",
    "\n",
    "Torch tensors are just like numpy arrays, but with a twist - they can also live on the GPU. Many methods are called similarly to numpy's, but unfortunately there are many API differences. See the documentation for supported functions: http://pytorch.org/docs/master/tensors.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use torch.form_numpy to convert a numpy array. For CPU tensors, it is fast (no memory copy)\n",
    "\n",
    "# Define 4 samples\n",
    "X = torch.from_numpy(np.array([[0,0],\n",
    "                               [0,1],\n",
    "                               [1,0],\n",
    "                               [1,1]], dtype=np.float32))\n",
    "Y = torch.from_numpy(np.array([[0], [1],[1], [0]], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  0\n",
      " 0  1\n",
      " 1  0\n",
      " 1  1\n",
      "[torch.FloatTensor of size 4x2]\n",
      " \n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 0\n",
      "[torch.FloatTensor of size 4x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,u'The XOR function')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAEICAYAAAC01Po2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOAjMzEFgZg4CM8NBYGY4CMwMB4GZAf8PqS/8abLW9koAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34f785a690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use tensor.numpy to turn a tensor into a numpy array - on CPU, this is fast (no memory copy)\n",
    "scatter(X.numpy()[:,0], X.numpy()[:,1], c=Y.numpy().ravel())\n",
    "axis('square')\n",
    "title('The XOR function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hW:  \n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      " hb:  \n",
      "-0.5000 -1.5000\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "H:  \n",
      " 0.0000  0.0000\n",
      " 1.0000  0.0000\n",
      " 1.0000  0.0000\n",
      " 1.0000  1.0000\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define two neurons: an AND neuron, and an OR neuron\n",
    "hW = torch.from_numpy(np.array([[1.0, 1.0],\n",
    "                                [1.0, 1.0]], dtype=np.float32))\n",
    "hb = torch.from_numpy(np.array([[-.5, -1.5]], dtype=np.float32))\n",
    "\n",
    "print('hW: ', hW, 'hb: ', hb)\n",
    "\n",
    "# compute the hidden activation, we multiply by 10. to have the sigmoid look like the step function\n",
    "H = torch.sigmoid(100 * (X.matmul(hW) + hb))\n",
    "\n",
    "print('H: ', H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O:  \n",
      " 1.9287e-22\n",
      " 1.0000e+00\n",
      " 1.0000e+00\n",
      " 1.9287e-22\n",
      "[torch.FloatTensor of size 4x1]\n",
      " Y:  \n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 0\n",
      "[torch.FloatTensor of size 4x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define an output neuron (x1 OR x2) AND NOT (x1 AND x2)\n",
    "\n",
    "oW = torch.from_numpy(np.array([[ 1.0], \n",
    "                                [-1.0]], dtype=np.float32))\n",
    "ob = torch.from_numpy(np.array([[-0.5]], dtype=np.float32))\n",
    "\n",
    "# compute the output, we multiply by 10. to have the sigmoid look like the step function\n",
    "O = torch.sigmoid(100.0 * (H.matmul(oW) + ob))\n",
    "\n",
    "print('O: ', O, 'Y: ', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0072\n",
       " 0.9924\n",
       " 0.9924\n",
       " 0.0072\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's define a function\n",
    "\n",
    "def net(X, hW, hb, oW, ob):\n",
    "    H = torch.sigmoid(10.0 * (X.matmul(hW) + hb))\n",
    "    O = torch.sigmoid(10.0 * (H.matmul(oW) + ob))\n",
    "    return O\n",
    "\n",
    "net(X, hW, hb, oW, ob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  0\n",
       " 0  1\n",
       " 1  0\n",
       " 1  1\n",
       "[torch.cuda.FloatTensor of size 4x2 (GPU 0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0072\n",
       " 0.9924\n",
       " 0.9924\n",
       " 0.0072\n",
       "[torch.cuda.FloatTensor of size 4x1 (GPU 0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CUDA computations\n",
    "# Use .cuda() to move variable to the GPU\n",
    "\n",
    "net(X.cuda(), hW.cuda(), hb.cuda(), oW.cuda(), ob.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Variable class know about gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XV:  Variable containing:\n",
      " 0  0\n",
      " 0  1\n",
      " 1  0\n",
      " 1  1\n",
      "[torch.cuda.FloatTensor of size 4x2 (GPU 0)]\n",
      "\n",
      "XV.data:  \n",
      " 0  0\n",
      " 0  1\n",
      " 1  0\n",
      " 1  1\n",
      "[torch.cuda.FloatTensor of size 4x2 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XV = Variable(X.cuda(), requires_grad=False)\n",
    "\n",
    "# .data holds the values\n",
    "print(\"XV: \", XV)\n",
    "print(\"XV.data: \", XV.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out Variable containing:\n",
      " 0.0072\n",
      " 0.9924\n",
      " 0.9924\n",
      " 0.0072\n",
      "[torch.FloatTensor of size 4x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XV = Variable(X, requires_grad=False)\n",
    "YV = Variable(Y, requires_grad=False)\n",
    "\n",
    "hWV = Variable(hW, requires_grad=True)\n",
    "hbV = Variable(hb, requires_grad=True)\n",
    "oWV = Variable(oW, requires_grad=True)\n",
    "obV = Variable(ob, requires_grad=True)\n",
    "\n",
    "# notice that out will also be a variable\n",
    "out = net(XV, hWV, hbV, oWV, obV)\n",
    "print (\"out\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-04 *\n",
       "  2.1919\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = ((out - YV)**2).sum()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  \n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "grad:  Variable containing:\n",
      "1.00000e-05 *\n",
      " -7.7096  0.9561\n",
      " -7.7096  0.9561\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .data holds the values\n",
    "print(\"data: \", hWV.data)\n",
    "\n",
    "# .grad holds the accumulated grdient, but \n",
    "print(\"grad: \", hWV.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.30230009556\n",
      "250 0.248610556126\n",
      "500 0.0135626578704\n",
      "750 0.006257802248\n",
      "1000 0.00399778736755\n",
      "1250 0.00291724829003\n",
      "1500 0.00228852662258\n",
      "1750 0.00187883758917\n",
      "2000 0.00159139977768\n",
      "2250 0.00137893657666\n",
      "2500 0.00121567759197\n",
      "2750 0.00108641514089\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# Depending on the initialization, this networks fails to train in about 10% cases.\n",
    "# We'll explain why during next lecture.\n",
    "params = [hWV, hbV, oWV, obV]\n",
    "\n",
    "for p in params:\n",
    "    p.data = torch.from_numpy(np.random.randn(*p.size()).astype('float32') * 0.1)\n",
    "\n",
    "for step in range(3000):\n",
    "    for p in params:\n",
    "        # functions ending in _ modify the tensor!\n",
    "        p.grad.data.zero_()\n",
    "    out = net(XV, *params)\n",
    "    loss = ((out - YV)**2).sum()\n",
    "    if (step % 250) == 0:\n",
    "        print(step, loss.data[0])\n",
    "    loss.backward()\n",
    "    for p in params:\n",
    "        p.data -= 1e-2 * p.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hWV,',\n",
      "  Variable containing:\n",
      " 0.4769 -0.6519\n",
      " 0.4772 -0.6535\n",
      "[torch.FloatTensor of size 2x2]\n",
      "),\n",
      " ('hbV,',\n",
      "  Variable containing:\n",
      "-0.7415  0.2595\n",
      "[torch.FloatTensor of size 1x2]\n",
      "),\n",
      " ('oWV,',\n",
      "  Variable containing:\n",
      "-1.0196\n",
      "-0.9877\n",
      "[torch.FloatTensor of size 2x1]\n",
      "),\n",
      " ('obV', Variable containing:\n",
      " 0.4962\n",
      "[torch.FloatTensor of size 1x1]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(zip ('hWV, hbV, oWV, obV'.split(), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 1:\n",
    "    for p in params:\n",
    "        # functions ending in _ modify the tensor!\n",
    "        p.grad.data.zero_()\n",
    "    out = net(XV, *params)\n",
    "    loss = ((out - YV)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.00571060181\n",
      "250 1.00000333786\n",
      "500 0.999988973141\n",
      "750 0.999966561794\n",
      "1000 0.999856650829\n",
      "1250 0.997202694416\n",
      "1500 0.301792025566\n",
      "1750 0.0391596369445\n",
      "2000 0.0190621707588\n",
      "2250 0.0123739829287\n",
      "2500 0.00909661035985\n",
      "2750 0.00716654770076\n"
     ]
    }
   ],
   "source": [
    "# Alt network - this one is easier to train and converges most of the time!\n",
    "params = [hWV, hbV, oWV, obV]\n",
    "\n",
    "def net2(X, hW, hb, oW, ob):\n",
    "    H = torch.tanh(((X * 2.0 - 1.0).matmul(hW) + hb))\n",
    "    O = torch.sigmoid((H.matmul(oW) + ob))\n",
    "    return O\n",
    "\n",
    "for p in params:\n",
    "    p.data = torch.from_numpy(np.random.randn(*p.size()).astype('float32') * 0.1)\n",
    "\n",
    "for step in range(3000):\n",
    "    for p in params:\n",
    "        # functions ending in _ modify the tensor!\n",
    "        p.grad.data.zero_()\n",
    "    out = net2(XV, *params)\n",
    "    loss = ((out - YV)**2).sum()\n",
    "    if (step % 250) == 0:\n",
    "        print(step, loss.data[0])\n",
    "    loss.backward()\n",
    "    for p in params:\n",
    "        p.data -= p.grad.data * 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
